{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"u0JbMXu9F1wN"},"outputs":[],"source":["# This notebook is used to\n","# 1. Refine the classifiers to mitigate data imbalance\n","# 2. Create ensemble classifier\n","# 3. Perform feature ablation\n","\n","import os\n","import pickle5\n","import pandas as pd\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import scipy.stats as stats\n","import copy\n","from random import sample\n","from joblib import dump, load\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n","from sklearn.decomposition import PCA\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn import metrics\n","from sklearn.metrics import classification_report, f1_score, fbeta_score, make_scorer, accuracy_score, confusion_matrix, plot_confusion_matrix, roc_auc_score, brier_score_loss\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, learning_curve, validation_curve\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","plt.style.use('bmh')\n","mpl.rcParams.update({\n","    \"grid.linestyle\" : \"dashed\",\n","    \"axes.facecolor\" : \"white\",\n","    \"axes.spines.top\" : False,\n","    \"axes.spines.right\" : False,\n","    \"legend.frameon\" : False,\n","    \"figure.figsize\" : (8, 5),\n","    \"figure.dpi\" : 300,\n","})\n","\n","# suppress sklearn deprecated warnings\n","import warnings\n","def warn(*args, **kwargs): pass\n","warnings.warn = warn"]},{"cell_type":"markdown","metadata":{"id":"_74Nc13rF1wM"},"source":["### Read Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vTvGW_RFf_GU"},"outputs":[],"source":["suffix_old = \"\"\n","suffix = \"\"\n","\n","# path = f\"drive/MyDrive/UNI/IPOTERI/data/cad/\"\n","# path_models = f\"{path}models/\"\n","n_features = 18\n","path = \"data/\"\n","path_models = f\"models/{n_features}features/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMED0-pgF1wO"},"outputs":[],"source":["# Read data\n","df_train = pd.read_csv(f\"{path}train{suffix}.csv\", index_col=0)\n","df_valid = pd.read_csv(f\"{path}valid{suffix_old}.csv\", index_col=0)\n","df_test = pd.read_csv(f\"{path}test{suffix_old}.csv\", index_col=0)\n","\n","if n_features == 7:\n","    top_variables = [\n","        \"Hyperlipemia\\nHistoty of hyperlipemia\",\n","        \"FE\",\n","        \"Previous CABG\",\n","        \"Diabetes\\nHistory of diabetes\",\n","        \"Previous Myocardial Infarction\",\n","        \"Smoke\\nHistory of smoke\",\n","        \"Documented resting \\nor exertional ischemia\",\n","        \"Survive7Y\"\n","    ]\n","    df_train = df_train.loc[:, top_variables]\n","    df_valid = df_valid.loc[:, top_variables]\n","    df_test = df_test.loc[:, top_variables]\n","\n","train, valid, test = df_train.to_numpy(), df_valid.to_numpy(), df_test.to_numpy()\n","X_train, y_train = train[:, :-1], train[:, -1]\n","X_valid, y_valid = valid[:, :-1], valid[:, -1]\n","X_test, y_test = test[:, :-1], test[:, -1]\n","feat_names = list(df_train.columns)\n","\n","from collections import Counter\n","print(Counter(y_train))\n","print(Counter(y_valid))\n","print(Counter(y_test))\n","\n","# All the numerical features that can be standarditazed\n","from utils import get_preprocess_std_num\n","preprocess_std = get_preprocess_std_num(feat_names)\n","preprocess_std_all = StandardScaler()\n","\n","# Preprocessed ready-to-use train and valid set\n","process_tmp = preprocess_std.fit(X_train)\n","X_train_std = process_tmp.transform(X_train)\n","X_valid_std = process_tmp.transform(X_valid)"]},{"cell_type":"markdown","metadata":{"id":"NDiNrGzPB1AH"},"source":["### Ensemble\n","Combining the previous best models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AOsEF1JZ23lk"},"outputs":[],"source":["from ensemble import build_ensemble, evaluate_ensemble, predict_ensemble\n","\n","models = [\n","    \"logreg_random_svmsmote_logreg\",\n","    # \"svc2_random_svmsmote_svc2\",\n","    # \"knn_random_svmsmote_knn\",\n","    \"rf1_random_bordersmote_rf1\",\n","    \"adaboost2_random_svmsmote_adaboost2\",\n","    # \"nn_random_svmsmote_nn\",\n","    # \"gb_random_svmsmote_gb\",\n","    # \"xgb_random_svmsmote_xgb\",\n","]\n","\n","# models_top7 = [\n","#     \"logreg_top_random_svmsmote_logreg_top\",\n","#     \"rf_top_random_svmsmote_rf_top\",\n","#     \"adaboost_top_random_svmsmote_adaboost_top\",\n","# ]\n","\n","ensemble = build_ensemble(models, path_models)\n","names = list(map(lambda x: x[0], ensemble))\n","ensemble = list(map(lambda x: x[1], ensemble))\n","evaluate_ensemble(ensemble, X_valid, y_valid)\n","evaluate_ensemble(ensemble, X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1YbZNg7Duli"},"outputs":[],"source":["train_prob, train_pred = predict_ensemble(ensemble, X_train, y_train)\n","valid_prob, valid_pred = predict_ensemble(ensemble, X_valid, y_valid)\n","test_prob, test_pred = predict_ensemble(ensemble, X_test, y_test)\n","\n","print(sum(y_train == train_pred) / len(df_train))\n","print(sum(y_valid == valid_pred) / len(df_valid))\n","print(sum(y_test == test_pred) / len(df_test))\n","\n","df_train[\"ModelOutput\"] = train_prob[:, 1]\n","df_valid[\"ModelOutput\"] = valid_prob[:, 1]\n","df_test[\"ModelOutput\"] = test_prob[:, 1]\n","# df_valid[\"ModelOutput\"].to_csv(\"extra_valid_output.csv\")\n","# df_test[\"ModelOutput\"].to_csv(\"extra_test_output.csv\")\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.calibration import calibration_curve\n","\n","true_pos, pred_pos = calibration_curve(y_test, test_prob[:, 1], n_bins=10)\n","plt.plot(pred_pos,\n","         true_pos, \n","         marker='o', \n","         linewidth=1, \n","         label='Ensemble')\n","plt.plot([0, 1], \n","         [0, 1], \n","         linestyle='--', \n","         label='Perfectly Calibrated')\n","\n","# plt.title('Probability Calibration Curve')\n","plt.xlabel('Predicted Probability')\n","plt.ylabel('True Probability')\n","plt.legend(loc='best')\n","\n","from PIL import Image\n","png2 = Image.open(\"figures/calibration1.png\")\n","png2.save(\"figures/calibration1.tiff\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_cvd = pd.read_csv(\"./data/raw/data_cvd.csv\", index_col=0, decimal='.')\n","df_cvd[\"Follow Up Data\"] = pd.to_datetime(df_cvd[\"Follow Up Data\"])\n","df_cvd[\"Data prelievo\"] = pd.to_datetime(df_cvd[\"Data prelievo\"])\n","df_cvd[\"Elapsed\"] = (df_cvd[\"Follow Up Data\"] - df_cvd[\"Data prelievo\"]).map(lambda x: x.days // 365)\n","df_plot = df_test.join(df_cvd[\"Elapsed\"])\n","\n","assert len(df_plot) == len(df_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(rasterized=True)\n","\n","boxes = list(map(lambda x: list(x[1].values), list(df_plot.groupby(\"Elapsed\")[\"ModelOutput\"])))\n","boxes = boxes[:-1] + [[]] + boxes[-1]\n","plt.boxplot(boxes, showfliers=False)\n","\n","n = 0.\n","noise = (np.random.rand(len(df_plot)) * n) - (n / 2)\n","plt.scatter(df_plot[\"Elapsed\"]+1+noise, df_plot[\"ModelOutput\"], c=df_plot[\"ModelOutput\"], alpha=0.5)\n","\n","plt.xlabel(\"Years\")\n","plt.ylabel(\"Predicted Probability\")\n","# plt.xticks(range(18), [f\"n={len(box)}\" for box in boxes])\n","\n","from PIL import Image\n","png2 = Image.open(\"figures/calibration.png\")\n","png2.save(\"figures/calibration.tiff\")"]},{"cell_type":"markdown","metadata":{"id":"6pcikGbdisuc"},"source":["### Sampling\n","Oversample and undersample methods to mitigate data imbalance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANhMfp3-gRgl"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n","from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN\n","from imblearn.under_sampling import RandomUnderSampler, OneSidedSelection, NeighbourhoodCleaningRule\n","from collections import Counter\n","\n","overs = [\n","    (\"smote\", SMOTE(sampling_strategy=1.0, k_neighbors=1)),\n","    (\"bordersmote\", BorderlineSMOTE(sampling_strategy=1.0, k_neighbors=1)),\n","    (\"svmsmote\", SVMSMOTE(sampling_strategy=1.0, k_neighbors=1)), \n","    # ADASYN(sampling_strategy=1.0, n_neighbors=1)\n","]\n","\n","random_ratio = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n","name_models = [\"logreg_top\", \"rf_top\", \"adaboost_top\"]\n","# name_models = [\"logreg\", \"svc2\", \"knn\", \"rf1\", \"adaboost2\", \"nn\", \"gb\", \"xgb\", \"xgb2\"]\n","\n","for name in name_models:\n","    best_k = 0\n","    best_over = \"\"\n","    best_score = 0\n","    best_random = 0\n","    best_model = None\n","    best_X, best_y = None, None\n","    model = load(path_models+f\"{name}{suffix}.joblib\")\n","\n","    for r in random_ratio:\n","        print(\"-------------------------------------------------\")\n","        print(f\"{'Oversample':<20}{'Score':<20}Random {r}\")\n","        print(\"-------------------------------------------------\")\n","\n","        for name_over, over in overs:\n","            for k in range(2, 5):\n","                over.set_params(k_neighbors=k)\n","                under = RandomUnderSampler(sampling_strategy=r)\n","                # under = OneSidedSelection(n_neighbors=100, n_seeds_S=300)\n","                # under = NeighbourhoodCleaningRule(n_neighbors=25, threshold_cleaning=0.5)\n","                X_train_sample, y_train_sample = under.fit_resample(X_train, y_train)\n","                X_train_sample, y_train_sample = over.fit_resample(X_train_sample, y_train_sample)\n","\n","                # Evaluate model with 5 runs\n","                scores = []\n","                for _ in range(5):\n","                    model.fit(X_train_sample, y_train_sample)\n","                    score = f1_score(y_valid, model.predict(X_valid), average=\"macro\")\n","                    scores.append(score)\n","                score = np.mean(scores)\n","\n","                if score > best_score:\n","                    best_k = k\n","                    best_over = name_over\n","                    best_score = score\n","                    best_random = r\n","                    best_model = copy.deepcopy(model)\n","                    best_X = X_train_sample\n","                    best_y = y_train_sample\n","\n","            print(f'{name_over:<20}{score:.3f}')\n","\n","    # Save the dataset and model\n","    combination = f\"{suffix}_random_{best_over}_{name}\"\n","    tmp = np.concatenate((best_X, np.expand_dims(best_y, 1)), axis=1)\n","    tmp = pd.DataFrame(tmp, columns=feat_names)\n","\n","    # Current model's statistics\n","    print(\"\\n\")\n","    print(f\"Name model:      \\t{name}\")\n","    print(f\"Best rand_ratio: \\t{best_random}\")\n","    print(f\"Best score:      \\t{best_score}\")\n","    print(f\"Dataset size:    \\t{len(best_y)}, {Counter(best_y)}\")\n","    print(f\"Combination:     \\t{combination}\")\n","    print(\"\\n\\n\")\n","\n","    # Evaluate the best model, save the data and the best model\n","    evaluate(best_model, best_X, best_y)\n","    evaluate(best_model, X_valid, y_valid)\n","    tmp.to_csv(path+f\"train{combination}.csv\")\n","    dump(best_model, path_models+f\"{name}{combination}.joblib\")\n","\n","\n","# save model\n","tmp.to_csv(path+f\"train{combination}.csv\")\n","dump(best_model, path_models+f\"{name}{combination}.joblib\")\n","\n","# evaluate the best model\n","evaluate(best_model, best_X, best_y)\n","evaluate(best_model, X_valid, y_valid)\n","tmp.to_csv(path+f\"train{combination}.csv\")\n","dump(best_model, path_models+f\"{name}{combination}.joblib\")"]},{"cell_type":"markdown","metadata":{"id":"MWNWoJwXZui5"},"source":["### Feature Ablation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vs8owbDoZxtg"},"outputs":[],"source":["from sklearn.inspection import permutation_importance\n","\n","\n","def get_univariate_ablation_results(X, y):\n","    \"\"\"For each feature we set it to its mean value and save the resulting metrics\"\"\"\n","\n","    features = df_valid.columns\n","    accuracy, auroc, f1_death, f1_survive, f1_macro = [], [], [], [], []\n","    for i, _ in enumerate(range(X_valid.shape[1])):\n","        X_copy = X.copy()\n","        X_copy[:, i] = np.mean(X_copy[:, i]) * np.ones(X_copy.shape[0])\n","        y_proba, y_pred = predict_ensemble(ensemble, X_copy, y)\n","\n","        accuracy.append((features[i], round(accuracy_score(y, y_pred), 3)))\n","        auroc.append((features[i], round(roc_auc_score(y, y_proba[:, 1]), 3)))\n","        f1_death.append((features[i], round(f1_score(y, y_pred, pos_label=0), 3)))\n","        f1_survive.append((features[i], round(f1_score(y, y_pred, pos_label=1), 3)))\n","        f1_macro.append((features[i], round(f1_score(y, y_pred, average=\"macro\"), 3)))\n","    \n","    return accuracy, auroc, f1_death, f1_survive, f1_macro\n","\n","\n","def get_multivariate_ablation_results(feat_clusters, X, y, skip_single=False):\n","    \"\"\"For each cluster of features, we set each feature of a group to its mean and save the resulting metrics\"\"\"\n","\n","    accuracy, auroc, f1_death, f1_survive, f1_macro = [], [], [], [], []\n","    for feat_cluster in feat_clusters:\n","        if skip_single and (len(feat_cluster)) == 1:\n","            continue\n","\n","        # Create a copy of the dataset\n","        X_copy = X.copy()\n","        t = []\n","        for feat in feat_cluster:\n","            p = feat_names.index(feat)\n","            t.append(p)\n","            X_copy[:, p] = np.mean(X_copy[:, p]) * np.ones(X_copy.shape[0])\n","\n","        # Predict\n","        y_proba, y_pred = predict_ensemble(ensemble, X_copy, y)\n","        accuracy.append(round(accuracy_score(y, y_pred), 3))\n","        auroc.append(round(roc_auc_score(y, y_proba[:, 1]), 3))\n","        f1_death.append(round(f1_score(y, y_pred, pos_label=0), 3))\n","        f1_survive.append(round(f1_score(y, y_pred, pos_label=1), 3))\n","        f1_macro.append(round(f1_score(y, y_pred, average=\"macro\"), 3))\n","    \n","    return accuracy, auroc, f1_death, f1_survive, f1_macro"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zljxZiD5LRQL"},"outputs":[],"source":["# Univariate ablation\n","accuracy, auroc, f1_death, f1_survive, f1_macro = get_univariate_ablation_results(X_test, y_test)\n","p = pd.DataFrame({\n","    \"auroc\": [x[1] for x in auroc], \n","    \"f1_macro\": [x[1] for x in f1_macro],\n","    }, index=feat_names[:-1])\n","p.to_csv(\"extra_ablation_uni_test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3pP7Gg_ByoJ"},"outputs":[],"source":["# Multivariate hierarchical ablation\n","with open(f\"{path}feat_cluster_hier.df\", \"rb\") as f:\n","    df_clusters = pickle5.load(f)\n","\n","accuracy, auroc, f1_death, f1_survive, f1_macro = get_multivariate_ablation_results(df_clusters, X_test, y_test)\n","p = pd.DataFrame({\"cluster\": df_clusters, \"auroc\": auroc, \"f1_macro\": f1_macro})\n","p.to_csv(\"multivariate_ablation_hier_nomeds.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-AfH3GQyUa5t"},"outputs":[],"source":["# # Correlation matrix multivariate ablation\n","# path_corr = f\"drive/MyDrive/UNI/IPOTERI/data/data/features_clustering/\"\n","# df_corr = pd.read_csv(f\"{path_corr}feat_cluster_0.4.csv\", index_col=0)\n","# df_corr = df_corr.iloc[:, 0].apply(lambda x: set(x.split(\",\")))\n","\n","# # Eliminate the subset feature clusters since they are redundant\n","# feat_clusters = []\n","# for i in range(len(df_corr)):\n","#     subset = False\n","#     for j in range(i+1, len(df_corr)):\n","#         if df_corr[i].issubset(df_corr[j]):\n","#             subset = True\n","#             break\n","\n","#     if not subset:\n","#         feat_clusters.append(list(df_corr[i]))\n","# print(f\"Number of clusters: {len(feat_clusters)}\")\n","\n","# # ablation results\n","# accuracy, auroc, f1_death, f1_survive, f1_macro = get_multivariate_ablation_results(feat_clusters, X_test, y_test, skip_single=True)\n","# ablation_results = [feat_clusters, accuracy, auroc, f1_survive, f1_death, f1_macro]\n","# p = pd.DataFrame(np.transpose(ablation_results))\n","# # p.to_csv(\"corr_ablation_nomeds.csv\")\n","# p.head()"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.10.0 ('ipoteri')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"vscode":{"interpreter":{"hash":"845b4840230c082c9fa3266fb7fe4179530e4b97f7408c449f411300982041c4"}}},"nbformat":4,"nbformat_minor":0}
