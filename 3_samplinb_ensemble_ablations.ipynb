{"cells":[{"cell_type":"markdown","metadata":{"id":"D42Y0zN2L9du"},"source":["This notebook is used to\n","1. Refine the classifiers to mitigate data imbalance\n","2. Create ensemble classifier\n","3. Perform feature ablation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2hy_zOGF209"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0JbMXu9F1wN"},"outputs":[],"source":["!pip3 install pickle5\n","\n","import os\n","import pickle5\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.stats as stats\n","import copy\n","from random import sample\n","from joblib import dump, load\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n","from sklearn.decomposition import PCA\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn import metrics\n","from sklearn.metrics import classification_report, f1_score, fbeta_score, make_scorer, accuracy_score, confusion_matrix, plot_confusion_matrix, roc_auc_score\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, learning_curve, validation_curve\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","plt.style.use('ggplot')\n","%matplotlib inline\n","\n","# suppress sklearn deprecated warnings\n","import warnings\n","def warn(*args, **kwargs): pass\n","warnings.warn = warn"]},{"cell_type":"markdown","metadata":{"id":"_74Nc13rF1wM"},"source":["### Read Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vTvGW_RFf_GU"},"outputs":[],"source":["# Save paths on drive for models\n","path = f\"drive/MyDrive/UNI/IPOTERI/data/cad/\"\n","path_models = f\"{path}models/\"\n","suffix_old = \"\"\n","suffix = \"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMED0-pgF1wO"},"outputs":[],"source":["# Read data\n","df_train = pd.read_csv(f\"{path}train{suffix}.csv\", index_col=0)\n","df_valid = pd.read_csv(f\"{path}valid{suffix_old}.csv\", index_col=0)\n","df_test = pd.read_csv(f\"{path}test{suffix_old}.csv\", index_col=0)\n","\n","# # Use only top 7 variables\n","# top_variables = [\n","#     \"Hyperlipemia\\nHistoty of hyperlipemia\",\n","#     \"FE\",\n","#     \"Previous CABG\",\n","#     \"Diabetes\\nHistory of diabetes\",\n","#     \"Previous Myocardial Infarction\",\n","#     \"Smoke\\nHistory of smoke\",\n","#     \"Documented resting \\nor exertional ischemia\",\n","#     \"Survive7Y\"\n","# ]\n","# df_train = df_train.loc[:, top_variables]\n","# df_valid = df_valid.loc[:, top_variables]\n","# df_test = df_test.loc[:, top_variables]\n","\n","train, valid, test = df_train.to_numpy(), df_valid.to_numpy(), df_test.to_numpy()\n","X_train, y_train = train[:, :-1], train[:, -1]\n","X_valid, y_valid = valid[:, :-1], valid[:, -1]\n","X_test, y_test = test[:, :-1], test[:, -1]\n","feat_names = list(df_train.columns)\n","\n","from collections import Counter\n","print(Counter(y_train))\n","print(Counter(y_valid))\n","print(Counter(y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zbNAdx1ey1r_"},"outputs":[],"source":["# All the numerical features that can be standarditazed\n","feat_names_num = [\"Age\", \"FE\", \"Creatinina\"]\n","\n","# Preprocess only the numerical features\n","def get_preprocess_std_num(feat_names):\n","    def update_num_feats(x):\n","        if x in feat_names:\n","            return feat_names.index(x)\n","\n","    num_feat_index = list(map(update_num_feats, feat_names_num))\n","    num_feat_index = [x for x in num_feat_index if x is not None]\n","    preprocess_std_num = ColumnTransformer(\n","                                transformers = [('stand', StandardScaler(), num_feat_index)], \n","                                remainder=\"passthrough\"\n","                            )\n","    return preprocess_std_num\n","\n","preprocess_std = get_preprocess_std_num(feat_names)\n","preprocess_std_all = StandardScaler()\n","\n","# Preprocessed ready-to-use train and valid set\n","process_tmp = preprocess_std.fit(X_train)\n","X_train_std = process_tmp.transform(X_train)\n","X_valid_std = process_tmp.transform(X_valid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5CvWOsgrF1wP"},"outputs":[],"source":["# Utility function to report best scores\n","def report(results, n_top=3):\n","    for i in range(1, n_top + 1):\n","        candidates = np.flatnonzero(results['rank_test_score'] == i)\n","        for candidate in candidates[:1]:\n","            print(\"Model rank: {0}\".format(i))\n","            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n","                  .format(results['mean_test_score'][candidate],\n","                          results['std_test_score'][candidate]))\n","            print(\"Parameters: {0}\".format(results['params'][candidate]))\n","            print(\"\")\n","            \n","\n","# Evaluate models\n","def evaluate(pipe, X, y, plot=False):\n","    y_pred = pipe.predict(X)\n","    print(classification_report(y, y_pred, digits=3))\n","    print(f\"auc macro {roc_auc_score(y, pipe.predict_proba(X)[:, 1]):.3f}\")\n","\n","    if plot:\n","        plot_confusion_matrix(pipe, X, y, normalize=None, values_format = '')\n","        plt.grid(False)\n","    else:\n","        print(\"confusion matrix\")\n","        print(confusion_matrix(y, y_pred))\n","\n","\n","# Train and evaluate\n","def train_and_evaluate(\n","    preprocess, \n","    model, \n","    hyperparams, \n","    X_train, \n","    y_train, \n","    X_valid, \n","    y_valid, \n","    scoring=\"f1_macro\", \n","    iter=5000, \n","    save=False, \n","    savename=\"\"\n","):\n","    pipe = Pipeline(steps=[\n","        ('preprocess', preprocess), \n","        ('model', model)\n","    ])\n","\n","    rand = RandomizedSearchCV(pipe,\n","                              param_distributions=hyperparams,\n","                              n_iter=iter,\n","                              scoring=scoring,\n","                              cv=2,\n","                              n_jobs=-1,    # use all processors\n","                              refit=True,   # refit the best model at the end\n","                              return_train_score=True,\n","                              verbose=True).fit(X_train, y_train)\n","    \n","    evaluate(rand.best_estimator_, X_train, y_train)\n","    evaluate(rand.best_estimator_, X_valid, y_valid)\n","    report(rand.cv_results_, n_top=5)\n","\n","    if save:\n","        dump(rand.best_estimator_, f\"{path_models}{savename}{suffix}.joblib\")\n","    \n","    return rand.best_estimator_"]},{"cell_type":"markdown","metadata":{"id":"6pcikGbdisuc"},"source":["### Sampling\n","Oversample and undersample methods to mitigate data imbalance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANhMfp3-gRgl"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n","from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN\n","from imblearn.under_sampling import RandomUnderSampler, OneSidedSelection, NeighbourhoodCleaningRule\n","from collections import Counter\n","\n","overs = [\n","    (\"smote\", SMOTE(sampling_strategy=1.0, k_neighbors=1)),\n","    (\"bordersmote\", BorderlineSMOTE(sampling_strategy=1.0, k_neighbors=1)),\n","    (\"svmsmote\", SVMSMOTE(sampling_strategy=1.0, k_neighbors=1)), \n","    # ADASYN(sampling_strategy=1.0, n_neighbors=1)\n","]\n","\n","random_ratio = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n","name_models = [\"logreg_top\", \"rf_top\", \"adaboost_top\"]\n","# name_models = [\"logreg\", \"svc2\", \"knn\", \"rf1\", \"adaboost2\", \"nn\", \"gb\", \"xgb\", \"xgb2\"]\n","\n","for name in name_models:\n","    best_k = 0\n","    best_over = \"\"\n","    best_score = 0\n","    best_random = 0\n","    best_model = None\n","    best_X, best_y = None, None\n","    model = load(path_models+f\"{name}{suffix}.joblib\")\n","\n","    for r in random_ratio:\n","        print(\"-------------------------------------------------\")\n","        print(f\"{'Oversample':<20}{'Score':<20}Random {r}\")\n","        print(\"-------------------------------------------------\")\n","\n","        for name_over, over in overs:\n","            for k in range(2, 5):\n","                over.set_params(k_neighbors=k)\n","                under = RandomUnderSampler(sampling_strategy=r)\n","                # under = OneSidedSelection(n_neighbors=100, n_seeds_S=300)\n","                # under = NeighbourhoodCleaningRule(n_neighbors=25, threshold_cleaning=0.5)\n","                X_train_sample, y_train_sample = under.fit_resample(X_train, y_train)\n","                X_train_sample, y_train_sample = over.fit_resample(X_train_sample, y_train_sample)\n","\n","                # Evaluate model with 5 runs\n","                scores = []\n","                for _ in range(5):\n","                    model.fit(X_train_sample, y_train_sample)\n","                    score = f1_score(y_valid, model.predict(X_valid), average=\"macro\")\n","                    scores.append(score)\n","                score = np.mean(scores)\n","\n","                if score > best_score:\n","                    best_k = k\n","                    best_over = name_over\n","                    best_score = score\n","                    best_random = r\n","                    best_model = copy.deepcopy(model)\n","                    best_X = X_train_sample\n","                    best_y = y_train_sample\n","\n","            print(f'{name_over:<20}{score:.3f}')\n","\n","    # Save the dataset and model\n","    combination = f\"{suffix}_random_{best_over}_{name}\"\n","    tmp = np.concatenate((best_X, np.expand_dims(best_y, 1)), axis=1)\n","    tmp = pd.DataFrame(tmp, columns=feat_names)\n","\n","    # Current model's statistics\n","    print(\"\\n\")\n","    print(f\"Name model:      \\t{name}\")\n","    print(f\"Best rand_ratio: \\t{best_random}\")\n","    print(f\"Best score:      \\t{best_score}\")\n","    print(f\"Dataset size:    \\t{len(best_y)}, {Counter(best_y)}\")\n","    print(f\"Combination:     \\t{combination}\")\n","    print(\"\\n\\n\")\n","\n","    # Evaluate the best model, save the data and the best model\n","    evaluate(best_model, best_X, best_y)\n","    evaluate(best_model, X_valid, y_valid)\n","    tmp.to_csv(path+f\"train{combination}.csv\")\n","    dump(best_model, path_models+f\"{name}{combination}.joblib\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZtv6MqV5lji"},"outputs":[],"source":["tmp.to_csv(path+f\"train{combination}.csv\")\n","dump(best_model, path_models+f\"{name}{combination}.joblib\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8vpNQ1K9MAO"},"outputs":[],"source":["# evaluate the best model\n","evaluate(best_model, best_X, best_y)\n","evaluate(best_model, X_valid, y_valid)\n","tmp.to_csv(path+f\"train{combination}.csv\")\n","dump(best_model, path_models+f\"{name}{combination}.joblib\")"]},{"cell_type":"markdown","metadata":{"id":"NDiNrGzPB1AH"},"source":["### Ensemble\n","Combining the previous best models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"78H0oSSKJPlx"},"outputs":[],"source":["def build_ensemble(models):\n","    ensemble = []\n","    for m in models:\n","        ensemble.append((m, load(path_models+f\"{m}.joblib\")))\n","    \n","    return ensemble\n","\n","\n","def predict_ensemble(ensemble, X, y, threshold=0.5):\n","    y_proba = []\n","    for m in ensemble:\n","        y_proba.append(m.predict_proba(X))\n","    y_proba = np.mean(y_proba, axis=0)\n","    y_pred = y_proba[:, 1] > threshold\n","    \n","    return y_proba, y_pred\n","\n","\n","def evaluate_ensemble(ensemble, X, y, threshold=0.5, verbose=True):\n","    y_proba, y_pred = predict_ensemble(ensemble, X, y)\n","    if verbose:\n","        print(classification_report(y, y_pred, digits=3))\n","        print(f\"auroc {roc_auc_score(y, y_proba[:, 1]):.3f} \\n\")\n","        print(\"confusion matrix\")\n","        print(confusion_matrix(y, y_pred))\n","  \n","    return f1_score(y, y_pred)\n","\n","\n","def find_best_ensemble(ensemble, X_valid, y_valid):\n","    results = []\n","    while len(ensemble) > 1:\n","        tmp_res = []\n","        for m in ensemble:\n","            tmp = copy.copy(ensemble)\n","            tmp.remove(m)\n","            names = [_name for _name, _m in tmp]\n","            tmp = [_m for _name, _m in tmp]\n","            acc = evaluate_ensemble(tmp, X_valid, y_valid, verbose=False)\n","            results.append((names, tmp, acc))\n","            tmp_res.append((m, acc))\n","\n","        m, _ = max(tmp_res, key=lambda item:item[1])\n","        ensemble.remove(m)\n","        # print(m)\n","        \n","    return max(results, key=lambda item:item[2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AOsEF1JZ23lk"},"outputs":[],"source":["models = [\n","    \"logreg_random_svmsmote_logreg\",\n","    # \"svc2_random_svmsmote_svc2\",\n","    # \"knn_random_svmsmote_knn\",\n","    \"rf1_random_bordersmote_rf1\",\n","    \"adaboost2_random_svmsmote_adaboost2\",\n","    # \"nn_random_svmsmote_nn\",\n","    # \"gb_random_svmsmote_gb\",\n","    # \"xgb_random_svmsmote_xgb\",\n","]\n","\n","models = [\n","    \"logreg_top_random_svmsmote_logreg_top\",\n","    \"rf_top_random_svmsmote_rf_top\",\n","    \"adaboost_top_random_svmsmote_adaboost_top\",\n","]\n","\n","ensemble = build_ensemble(models)\n","# names, ensemble, _ = find_best_ensemble(ensemble, X_valid, y_valid)\n","names = list(map(lambda x: x[0], ensemble))\n","ensemble = list(map(lambda x: x[1], ensemble))\n","evaluate_ensemble(ensemble, X_valid, y_valid)\n","evaluate_ensemble(ensemble, X_test, y_test)\n","print(\"\\n\")\n","print(names)"]},{"cell_type":"markdown","metadata":{"id":"f7QxSERMDlb7"},"source":["### Predictions\n","Save the logit output (probabilities) for survival analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1YbZNg7Duli"},"outputs":[],"source":["# Ensemble predictions\n","train_prob, train_pred = predict_ensemble(ensemble, X_train, y_train)\n","valid_prob, valid_pred = predict_ensemble(ensemble, X_valid, y_valid)\n","test_prob, test_pred = predict_ensemble(ensemble, X_test, y_test)\n","\n","print(sum(df_train[\"Survive7Y\"] == train_pred) / len(df_train))\n","print(sum(df_valid[\"Survive7Y\"] == valid_pred) / len(df_valid))\n","print(sum(df_test[\"Survive7Y\"] == test_pred) / len(df_test))\n","\n","df_train[\"ModelOutput\"] = train_prob[:, 1]\n","df_valid[\"ModelOutput\"] = valid_prob[:, 1]\n","df_test[\"ModelOutput\"] = test_prob[:, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCcLQD1aEKAR"},"outputs":[],"source":["df_valid[\"ModelOutput\"].to_csv(\"extra_valid_output.csv\")\n","df_test[\"ModelOutput\"].to_csv(\"extra_test_output.csv\")\")"]},{"cell_type":"markdown","metadata":{"id":"MWNWoJwXZui5"},"source":["### Feature Ablation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vs8owbDoZxtg"},"outputs":[],"source":["from sklearn.inspection import permutation_importance\n","\n","\n","def get_univariate_ablation_results(X, y):\n","    \"\"\"For each feature we set it to its mean value and save the resulting metrics\"\"\"\n","\n","    features = df_valid.columns\n","    accuracy, auroc, f1_death, f1_survive, f1_macro = [], [], [], [], []\n","    for i, _ in enumerate(range(X_valid.shape[1])):\n","        X_copy = X.copy()\n","        X_copy[:, i] = np.mean(X_copy[:, i]) * np.ones(X_copy.shape[0])\n","        y_proba, y_pred = predict_ensemble(ensemble, X_copy, y)\n","\n","        accuracy.append((features[i], round(accuracy_score(y, y_pred), 3)))\n","        auroc.append((features[i], round(roc_auc_score(y, y_proba[:, 1]), 3)))\n","        f1_death.append((features[i], round(f1_score(y, y_pred, pos_label=0), 3)))\n","        f1_survive.append((features[i], round(f1_score(y, y_pred, pos_label=1), 3)))\n","        f1_macro.append((features[i], round(f1_score(y, y_pred, average=\"macro\"), 3)))\n","    \n","    return accuracy, auroc, f1_death, f1_survive, f1_macro\n","\n","\n","def get_multivariate_ablation_results(feat_clusters, X, y, skip_single=False):\n","    \"\"\"For each cluster of features, we set each feature of a group to its mean and save the resulting metrics\"\"\"\n","\n","    accuracy, auroc, f1_death, f1_survive, f1_macro = [], [], [], [], []\n","    for feat_cluster in feat_clusters:\n","        if skip_single and (len(feat_cluster)) == 1:\n","            continue\n","\n","        # Create a copy of the dataset\n","        X_copy = X.copy()\n","        t = []\n","        for feat in feat_cluster:\n","            p = feat_names.index(feat)\n","            t.append(p)\n","            X_copy[:, p] = np.mean(X_copy[:, p]) * np.ones(X_copy.shape[0])\n","\n","        # Predict\n","        y_proba, y_pred = predict_ensemble(ensemble, X_copy, y)\n","        accuracy.append(round(accuracy_score(y, y_pred), 3))\n","        auroc.append(round(roc_auc_score(y, y_proba[:, 1]), 3))\n","        f1_death.append(round(f1_score(y, y_pred, pos_label=0), 3))\n","        f1_survive.append(round(f1_score(y, y_pred, pos_label=1), 3))\n","        f1_macro.append(round(f1_score(y, y_pred, average=\"macro\"), 3))\n","    \n","    return accuracy, auroc, f1_death, f1_survive, f1_macro"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zljxZiD5LRQL"},"outputs":[],"source":["# Univariate ablation\n","accuracy, auroc, f1_death, f1_survive, f1_macro = get_univariate_ablation_results(X_test, y_test)\n","p = pd.DataFrame({\n","    \"auroc\": [x[1] for x in auroc], \n","    \"f1_macro\": [x[1] for x in f1_macro],\n","    }, index=feat_names[:-1])\n","p.to_csv(\"extra_ablation_uni_test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3pP7Gg_ByoJ"},"outputs":[],"source":["# Multivariate hierarchical ablation\n","with open(f\"{path}feat_cluster_hier.df\", \"rb\") as f:\n","    df_clusters = pickle5.load(f)\n","\n","accuracy, auroc, f1_death, f1_survive, f1_macro = get_multivariate_ablation_results(df_clusters, X_test, y_test)\n","p = pd.DataFrame({\"cluster\": df_clusters, \"auroc\": auroc, \"f1_macro\": f1_macro})\n","p.to_csv(\"multivariate_ablation_hier_nomeds.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-AfH3GQyUa5t"},"outputs":[],"source":["# # Correlation matrix multivariate ablation\n","# path_corr = f\"drive/MyDrive/UNI/IPOTERI/data/data/features_clustering/\"\n","# df_corr = pd.read_csv(f\"{path_corr}feat_cluster_0.4.csv\", index_col=0)\n","# df_corr = df_corr.iloc[:, 0].apply(lambda x: set(x.split(\",\")))\n","\n","# # Eliminate the subset feature clusters since they are redundant\n","# feat_clusters = []\n","# for i in range(len(df_corr)):\n","#     subset = False\n","#     for j in range(i+1, len(df_corr)):\n","#         if df_corr[i].issubset(df_corr[j]):\n","#             subset = True\n","#             break\n","\n","#     if not subset:\n","#         feat_clusters.append(list(df_corr[i]))\n","# print(f\"Number of clusters: {len(feat_clusters)}\")\n","\n","# # ablation results\n","# accuracy, auroc, f1_death, f1_survive, f1_macro = get_multivariate_ablation_results(feat_clusters, X_test, y_test, skip_single=True)\n","# ablation_results = [feat_clusters, accuracy, auroc, f1_survive, f1_death, f1_macro]\n","# p = pd.DataFrame(np.transpose(ablation_results))\n","# # p.to_csv(\"corr_ablation_nomeds.csv\")\n","# p.head()"]},{"cell_type":"markdown","metadata":{"id":"N7A-SJXI4NNe"},"source":["### Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yRBdL8ig4JaK"},"outputs":[],"source":["name = f\"xgb2.joblib\"\n","model = load(path_models+name)\n","\n","# model.fit(X_train, y_train)\n","# evaluate(model, X_train, y_train)\n","evaluate(model, X_valid, y_valid)\n","evaluate(model, X_test, y_test)\n","X_train.shape"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.10.0 ('ipoteri')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"vscode":{"interpreter":{"hash":"845b4840230c082c9fa3266fb7fe4179530e4b97f7408c449f411300982041c4"}}},"nbformat":4,"nbformat_minor":0}
