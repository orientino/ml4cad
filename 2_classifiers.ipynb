{"cells":[{"cell_type":"markdown","metadata":{"id":"_74Nc13rF1wM"},"source":["This notebook is used to train classifiers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2hy_zOGF209"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0JbMXu9F1wN"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.stats as stats\n","import copy\n","from random import sample\n","from joblib import dump, load\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n","from sklearn.decomposition import PCA\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn import metrics\n","from sklearn.metrics import classification_report, f1_score, fbeta_score, make_scorer, accuracy_score, confusion_matrix, plot_confusion_matrix, roc_auc_score\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, learning_curve, validation_curve\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","plt.style.use('ggplot')\n","%matplotlib inline\n","\n","# Suppress sklearn deprecated warnings\n","import warnings\n","def warn(*args, **kwargs): pass\n","warnings.warn = warn\n","\n","np.random.seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BWAuNPuxEJ4e"},"outputs":[],"source":["# Save paths on drive for models\n","path = f\"drive/MyDrive/UNI/IPOTERI/data/cad/\"\n","path_models = f\"{path}models/\"\n","suffix_old = \"\"\n","suffix = \"\""]},{"cell_type":"markdown","source":["### Read Data"],"metadata":{"id":"5mq-Ql-XI7Or"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jUynZcMcywO6"},"outputs":[],"source":["# Read data\n","df_train = pd.read_csv(f\"{path}train{suffix}.csv\", index_col=0)\n","df_valid = pd.read_csv(f\"{path}valid{suffix_old}.csv\", index_col=0)\n","df_test = pd.read_csv(f\"{path}test{suffix_old}.csv\", index_col=0)\n","\n","# # Use only top 7 variables\n","# top_variables = [\n","#     \"Hyperlipemia\\nHistoty of hyperlipemia\",\n","#     \"FE\",\n","#     \"Previous CABG\",\n","#     \"Diabetes\\nHistory of diabetes\",\n","#     \"Previous Myocardial Infarction\",\n","#     \"Smoke\\nHistory of smoke\",\n","#     \"Documented resting \\nor exertional ischemia\",\n","#     \"Survive7Y\"\n","# ]\n","# df_train = df_train.loc[:, top_variables]\n","# df_valid = df_valid.loc[:, top_variables]\n","# df_test = df_test.loc[:, top_variables]\n","\n","train, valid, test = df_train.to_numpy(), df_valid.to_numpy(), df_test.to_numpy()\n","X_train, y_train = train[:, :-1], train[:, -1]\n","X_valid, y_valid = valid[:, :-1], valid[:, -1]\n","X_test, y_test = test[:, :-1], test[:, -1]\n","feat_names = list(df_train.columns)\n","\n","from collections import Counter\n","print(Counter(y_train))\n","print(Counter(y_valid))\n","print(Counter(y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vn7M8uzGyiFQ"},"outputs":[],"source":["# All the numerical features that can be standarditazed\n","feat_names_num = [\"Age\", \"FE\", \"Creatinina\"]\n","\n","# Preprocess only the numerical features\n","def get_preprocess_std_num(feat_names):\n","    def update_num_feats(x):\n","        if x in feat_names:\n","            return feat_names.index(x)\n","\n","    num_feat_index = list(map(update_num_feats, feat_names_num))\n","    num_feat_index = [x for x in num_feat_index if x is not None]\n","    preprocess_std_num = ColumnTransformer(\n","                                transformers = [('stand', StandardScaler(), num_feat_index)], \n","                                remainder=\"passthrough\"\n","                            )\n","    return preprocess_std_num\n","\n","preprocess_std = get_preprocess_std_num(feat_names)\n","preprocess_std_all = StandardScaler()\n","\n","# Preprocessed ready-to-use train and valid set\n","process_tmp = preprocess_std.fit(X_train)\n","X_train_std = process_tmp.transform(X_train)\n","X_valid_std = process_tmp.transform(X_valid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5CvWOsgrF1wP"},"outputs":[],"source":["# Utility function to report best scores\n","def report(results, n_top=3):\n","    for i in range(1, n_top + 1):\n","        candidates = np.flatnonzero(results['rank_test_score'] == i)\n","        for candidate in candidates[:1]:\n","            print(\"Model rank: {0}\".format(i))\n","            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n","                  .format(results['mean_test_score'][candidate],\n","                          results['std_test_score'][candidate]))\n","            print(\"Parameters: {0}\".format(results['params'][candidate]))\n","            print(\"\")\n","            \n","\n","# Evaluate models\n","def evaluate(pipe, X, y, plot=False):\n","    y_pred = pipe.predict(X)\n","    print(classification_report(y, y_pred, digits=3))\n","    print(f\"auc macro {roc_auc_score(y, pipe.predict_proba(X)[:, 1]):.3f}\")\n","\n","    if plot:\n","        plot_confusion_matrix(pipe, X, y, normalize=None, values_format = '')\n","        plt.grid(False)\n","    else:\n","        print(\"confusion matrix\")\n","        print(confusion_matrix(y, y_pred))\n","\n","\n","# Train and evaluate\n","def train_and_evaluate(\n","    preprocess, \n","    model, \n","    hyperparams, \n","    X_train, \n","    y_train, \n","    X_valid, \n","    y_valid, \n","    scoring=\"f1_macro\", \n","    iter=5000, \n","    save=False, \n","    savename=\"\"\n","):\n","    pipe = Pipeline(steps=[\n","        ('preprocess', preprocess), \n","        ('model', model)\n","    ])\n","\n","    rand = RandomizedSearchCV(pipe,\n","                              param_distributions=hyperparams,\n","                              n_iter=iter,\n","                              scoring=scoring,\n","                              cv=2,\n","                              n_jobs=-1,    # use all processors\n","                              refit=True,   # refit the best model at the end\n","                              return_train_score=True,\n","                              verbose=True).fit(X_train, y_train)\n","    \n","    evaluate(rand.best_estimator_, X_train, y_train)\n","    evaluate(rand.best_estimator_, X_valid, y_valid)\n","    report(rand.cv_results_, n_top=5)\n","\n","    if save:\n","        dump(rand.best_estimator_, f\"{path_models}{savename}{suffix}.joblib\")\n","    \n","    return rand.best_estimator_"]},{"cell_type":"markdown","metadata":{"id":"kPM0SyWhbsrK"},"source":["### Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UWdEgh6JF1wQ"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","\n","hyperparams = {\n","    'model__penalty': ['l1', 'l2', 'elasticnet'],\n","    'model__dual': [True, False],\n","    'model__warm_start': [True, False],\n","    'model__C': stats.randint(1, 10),\n","    'model__max_iter': stats.randint(50, 500),\n","    'model__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n","}\n","\n","model = LogisticRegression(class_weight=\"balanced\")\n","train_and_evaluate(preprocess_std, model, hyperparams, X_train, y_train, X_valid, y_valid, scoring=\"f1_macro\", iter=5000, save=True, savename=f\"logreg2\")\n","# metrics.plot_roc_curve(pipe, X_valid, y_valid)\n","\n","# import math\n","# w = logreg.coef_[0]\n","# feature_importance = pd.DataFrame(df_feat.columns[:-1], columns=[\"features\"])\n","# feature_importance[\"importance\"] = pow(math.e, w)\n","# feature_importance = feature_importance.sort_values(by = [\"importance\"], ascending=False)\n","# feature_importance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gu-nYOCSp90A"},"outputs":[],"source":["from sklearn.svm import SVC\n","\n","hyperparams = {\n","    'model__C': stats.randint(100, 600),\n","    'model__kernel': ['rbf', 'poly', 'sigmoid'],\n","    'model__degree': stats.randint(5, 200),\n","    'model__gamma': ['scale', 'auto'],\n","    'model__coef0': stats.uniform(0.0, 1),\n","    'model__max_iter': [400, 800, 1200, 1600]\n","}\n","\n","model = SVC(class_weight=\"balanced\", probability=True)\n","train_and_evaluate(preprocess_std, model, hyperparams, X_train, y_train, X_valid, y_valid, scoring=\"f1_macro\", iter=5000, save=True, savename=f\"svc2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptgdxc7SF1wS"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","hyperparams = {\n","    'model__n_neighbors': stats.randint(2, 100),\n","    'model__weights': ('uniform', 'distance'),\n","    'model__algorithm': ('ball_tree', 'kd_tree'),\n","    'model__leaf_size': stats.randint(10, 60)\n","}\n","\n","model = KNeighborsClassifier()\n","train_and_evaluate(preprocess_std, model, hyperparams, X_train, y_train, X_valid, y_valid, scoring=\"f1_macro\", iter=5000, save=True, savename=f\"knn2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U8yfkbGvF1wR"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","hyperparams = {\n","    'model__n_estimators': stats.randint(10, 200),\n","    'model__criterion': ('gini', 'entropy'),\n","    'model__min_samples_split': stats.randint(1, 8),\n","    'model__min_samples_leaf': stats.randint(1, 5),\n","    'model__max_features': ('sqrt', 'log2', None),\n","    'model__class_weight': ['balanced', 'balanced_subsample'],\n","}\n","\n","model = RandomForestClassifier()\n","train_and_evaluate(preprocess_std, model, hyperparams, X_train, y_train, X_valid, y_valid, scoring=\"f1_macro\", iter=5000, save=True, savename=f\"rf2\")\n","\n","# feature importance use permutation importance\n","# importance = rf_rand.best_estimator_[\"model\"].feature_importances_\n","# plt.bar(list(range(len(importance))), importance)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YAmHgqUkX7Lf"},"outputs":[],"source":["from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","hyperparams = {\n","    'model__n_estimators': stats.randint(10, 100),\n","    'model__learning_rate': stats.uniform(0.2, 1)\n","}\n","\n","model = AdaBoostClassifier()\n","train_and_evaluate(preprocess_std, model, hyperparams, X_train, y_train, X_valid, y_valid, scoring=\"f1_macro\", iter=5000, save=True, savename=f\"adaboost2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUsKDdQ8F1wT"},"outputs":[],"source":["from sklearn.neural_network import MLPClassifier\n","import random\n","\n","hyperparams = {\n","    'model__hidden_layer_sizes': [[stats.randint.rvs(100, 300), stats.randint.rvs(50, 150)], [stats.randint.rvs(50, 300)]],\n","    'model__solver': ['sgd', 'adam'],\n","    'model__learning_rate_init': stats.uniform(0.0005, 0.005),\n","    'model__learning_rate': ('constant', 'adaptive'),\n","    'model__alpha': stats.uniform(0, 1),\n","    'model__early_stopping': [True],\n","    'model__max_iter': stats.randint(300, 500),\n","}\n","\n","model = MLPClassifier()\n","train_and_evaluate(preprocess_std, model, hyperparams, X_train, y_train, X_valid, y_valid, scoring=\"f1_macro\", iter=2500, save=True, savename=f\"nn2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WROWp60CF1wS"},"outputs":[],"source":["from sklearn.ensemble import GradientBoostingClassifier\n","\n","hyperparams = {\n","    'model__learning_rate': stats.uniform(0.03, 0.2),\n","    'model__n_estimators': stats.randint(10, 100),\n","    'model__max_depth': stats.randint(2, 6),\n","    'model__max_features': ('sqrt', 'log2', None),  # regularization\n","    'model__subsample': (0.25, 0.5, 0.75, 1),       # regularization\n","}\n","\n","model = GradientBoostingClassifier()\n","train_and_evaluate(preprocess_std, model, hyperparams, X_train, y_train, X_valid, y_valid, scoring=\"f1_macro\", iter=5000, save=True, savename=f\"gb2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOc7qUYqqy_8"},"outputs":[],"source":["import xgboost as xgb\n","\n","hyperparams = {\n","    'model__booster': ['gbtree', 'gblinear', 'dart'],\n","    'model__eta': stats.uniform(0.05, 0.5),\n","    'model__gamma': stats.uniform(0, 0.2),\n","    'model__max_depth': [2, 3, 4, 6],\n","    'model__n_estimators': stats.randint(10, 100),\n","    'model__subsample': [0.25, 0.5, 0.75, 1],     # Stochastic regularization\n","    'model__lambda': stats.uniform(0.5, 1.5),     # L2 regularization\n","    'model__alpha': stats.uniform(0, 0.5),        # L1 regularization\n","    'model__scale_pos_weight': [0.2, 0.4, 0.8, 1, 2],\n","}\n","\n","model = xgb.XGBClassifier(n_jobs=1)\n","train_and_evaluate(preprocess_std, model, hyperparams, X_train, y_train, X_valid, y_valid, scoring=\"f1_macro\", iter=5, save=False, savename=f\"xgb2\")"]},{"cell_type":"markdown","metadata":{"id":"0i7FJyVlnRyT"},"source":["### Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtuJCvgenRQq"},"outputs":[],"source":["models = [\n","    \"logreg_random_svmsmote_logreg\",\n","    \"svc_random_bordersmote_svc\",\n","    \"knn2_random_svmsmote_knn2\",\n","    \"rf2_random_svmsmote_rf2\",\n","    \"adaboost2_random_svmsmote_adaboost2\",\n","    \"nn_random_svmsmote_nn\",\n","    \"gb_random_svmsmote_gb\",\n","    \"xgb2_random_svmsmote_xgb2\",\n","    \"xgb2\",\n","    \"nn1_random_svmsmote_nn1\",\n","]\n","\n","name = f\"{models[9]}.joblib\"\n","model = load(path_models+name)\n","\n","# model.fit(X_train, y_train)\n","# evaluate(model, X_train, y_train)\n","evaluate(model, X_valid, y_valid)\n","evaluate(model, X_test, y_test)\n","X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZAuqF0mGDq-"},"outputs":[],"source":["tree = [load(path_models+f\"tree4.joblib\")]\n","evaluate(tree[0], X_valid4, y_valid4)\n","evaluate(tree[0], X_test4, y_test4)"]},{"cell_type":"markdown","metadata":{"id":"ZdeFH5bkIbX5"},"source":["### Feature Selection\n","Testing with algorithms such as RFECV and SFS.\n","- RFECV select features by recursively considering smaller sets of features using models' feature importance.\n","- SFS greedy procedure that initially start with zero feature and find the one feature that maximizes a cross-validated score when an estimator is trained on this single feature. Repeat the procedure by adding a new feature to the set of selected features. The procedure stops when the desired number of selected features is reached (n_features_to_select)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jnmkTSNRpNcx"},"outputs":[],"source":["# !pip uninstall scikit-learn -y\n","# !pip install -U scikit-learn\n","# !pip install imblearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0Gv38Cuuweo"},"outputs":[],"source":["# # feature selection\n","# from sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, chi2\n","# selector = SelectKBest(chi2, k=15)\n","# selector.fit(X_train, y_train)\n","# # scores = -np.log10(selector.pvalues_)\n","# # scores /= scores.max()\n","# # plt.bar(range(train.shape[-1]-1), scores)\n","# plt.bar(range(train.shape[-1]-1), selector.pvalues_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CjNE3e_MiR6m"},"outputs":[],"source":["# from sklearn.feature_selection import RFECV, SequentialFeatureSelector\n","\n","# # apply the feature selection on the dataset\n","# model = load(path_models+\"logreg_60feats.joblib\")\n","# # selector = RFECV(model[\"model\"], min_features_to_select=1, cv=4, scoring=\"f1_macro\")\n","# selector = SequentialFeatureSelector(model[\"model\"], n_features_to_select=50, direction=\"backward\", cv=2, scoring=\"f1_macro\")\n","# selector = selector.fit(X_train_std, y_train)\n","# X_train_select = selector.transform(X_train)\n","# X_valid_select = selector.transform(X_valid)\n","# X_test_select = selector.transform(X_test)\n","\n","# # saving the feature selected dataset\n","# suffix = \"_60feats_SFS50_logreg\"\n","# df_train_select = pd.DataFrame(np.concatenate((X_train_select, np.expand_dims(y_train, 1)), axis=1), columns=new_feat_names)\n","# df_valid_select = pd.DataFrame(np.concatenate((X_valid_select, np.expand_dims(y_valid, 1)), axis=1), columns=new_feat_names)\n","# df_test_select = pd.DataFrame(np.concatenate((X_test_select, np.expand_dims(y_test, 1)), axis=1), columns=new_feat_names)\n","# # df_train_select.to_csv(path+f\"train{suffix}.csv\")\n","# # df_valid_select.to_csv(path+f\"valid{suffix}.csv\")\n","# # df_test_select.to_csv(path+f\"test{suffix}.csv\")\n","\n","# new_feat_names = list(pd.Index(feat_names[:-1])[selector.get_support()])\n","# new_feat_names.append(feat_names[-1])\n","# print(X_train_select.shape)\n","# print(new_feat_names)"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"toc_visible":true},"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"Python 3.8.10 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}