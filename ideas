Deal with unbalanced data:
- accuracy is misleading, use different performance metric: CM, P, R, F1, Kappa, AUC
- set the threshold yourself, let the model output logits/prob
- resampling: oversampling, undersampling
    - oversampling: random repeat, SMOTE, SPIDER (?)
    - undersampling: not enough data
- penalize model: add class weight in the optimizer
- decision trees C4.5, C5.0, Cart, Random Forest, can also handle missing values


https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set
    - minibatch -> sample at least one sample from the small class per batch
    - SGD training -> for rare class take more steps at each iteration (same as penalizing)
    - SGD training -> oversample but not too much
    - divide the abundand class into L clusters. Train L predictors and output the averages
    - divide the abundand class into L clusters. Train a predictor using the mediods of each cluster
    - each model has pro and cons, average them all!

References:
    https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/
    https://www.ele.uri.edu/faculty/he/PDFfiles/ImbalancedLearning.pdf

